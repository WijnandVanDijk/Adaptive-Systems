{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2bff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66504d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \"\"\"Represents the maze environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the maze with the map, dimensions, starting state, terminal states, and actions.\"\"\"\n",
    "        self.map = [[-1, -1, -1, 40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [10, -2, -1, -1]]\n",
    "        self.num_rows = len(self.map)\n",
    "        self.num_cols = len(self.map[0])\n",
    "        self.start_state = (3, 2)  # row, col\n",
    "        self.terminal_states = [(0, 3), (3, 0)]\n",
    "        self.agent_pos = self.start_state\n",
    "        self.actions = {'↑': (-1, 0), '→': (0, 1), '↓': (1, 0), '←': (0, -1)}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the given action.\n",
    "\n",
    "        Args:\n",
    "            action (str): The action to take.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The next state, reward, and done flag.\n",
    "        \"\"\"\n",
    "        move = self.actions[action]\n",
    "        row, col = self.agent_pos\n",
    "        new_row, new_col = row + move[0], col + move[1]\n",
    "\n",
    "        # Check if new location is within map boundaries\n",
    "        if (0 <= new_row < self.num_rows) and (0 <= new_col < self.num_cols):\n",
    "            self.agent_pos = (new_row, new_col)\n",
    "            reward = self.map[new_row][new_col]\n",
    "            done = self.agent_pos in self.terminal_states\n",
    "        else:\n",
    "            # If the new location is outside the map, stay in the same spot and get the same punishment again\n",
    "            reward = self.map[row][col]\n",
    "            done = False\n",
    "\n",
    "        return self.agent_pos, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b320a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Represents the agent interacting with the maze environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the agent with the maze, policy, and value function.\"\"\"\n",
    "        self.maze = Maze()\n",
    "        self.policy = Policy(self.maze)\n",
    "        self.value_function = {}\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Perform an action in the environment based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): The current state.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The next state, reward, and done flag.\n",
    "        \"\"\"\n",
    "        action = self.policy.select_action(state)\n",
    "        next_state, reward, done = self.maze.step(action)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b52a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"Represents the policy for selecting actions.\"\"\"\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        \"\"\"Initialize the policy with the available actions.\n",
    "\n",
    "        Args:\n",
    "            maze (Maze): The maze environment.\n",
    "        \"\"\"\n",
    "        self.actions = {'↑': (-1, 0), '→': (0, 1), '↓': (1, 0), '←': (0, -1)}\n",
    "        self.maze = maze\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Selects an action to take based on the current state.\n",
    "\n",
    "        This method randomly selects an action from the available actions defined in the policy.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): The current state.\n",
    "\n",
    "        Returns:\n",
    "            str: The selected action.\n",
    "        \"\"\"\n",
    "        return random.choice(list(self.actions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d40be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1\n",
      "delta=40.0\n",
      "Utility:\n",
      "-1.0 -1.0 40.0    0 \n",
      "-1.0 -1.0 39.0 40.0 \n",
      "10.0  9.0 29.0 30.0 \n",
      "   0 10.0 28.0 29.0 \n",
      "Policy:\n",
      "→ → ↓ ○ \n",
      "↓ → ↑ ← \n",
      "→ → ↑ ↑ \n",
      "○ → ↑ ↑ \n",
      "k=2\n",
      "delta=40.0\n",
      "Utility:\n",
      "-2.0 39.0 40.0    0 \n",
      " 9.0 38.0 39.0 40.0 \n",
      "10.0 37.0 36.0 35.0 \n",
      "   0 36.0 35.0 34.0 \n",
      "Policy:\n",
      "→ → ↓ ○ \n",
      "→ ↑ ↑ ← \n",
      "→ ↑ ↑ ↑ \n",
      "○ ↑ ↑ ↑ \n",
      "k=3\n",
      "delta=40.0\n",
      "Utility:\n",
      "38.0 39.0 40.0    0 \n",
      "37.0 38.0 39.0 40.0 \n",
      "36.0 37.0 36.0 35.0 \n",
      "   0 36.0 35.0 34.0 \n",
      "Policy:\n",
      "→ → ↓ ○ \n",
      "↑ ↑ ↑ ← \n",
      "↑ ↑ ↑ ↑ \n",
      "○ ↑ ↑ ↑ \n",
      "k=4\n",
      "delta=0\n",
      "Utility:\n",
      "38.0 39.0 40.0    0 \n",
      "37.0 38.0 39.0 40.0 \n",
      "36.0 37.0 36.0 35.0 \n",
      "   0 36.0 35.0 34.0 \n",
      "Policy:\n",
      "→ → ↓ ○ \n",
      "↑ ↑ ↑ ← \n",
      "↑ ↑ ↑ ↑ \n",
      "○ ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "def value_iteration(agent, delta=0.01, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Perform value iteration to calculate the optimal value function.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): The agent interacting with the environment.\n",
    "        delta (float): The threshold for convergence.\n",
    "        gamma (float): The discount factor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the value function\n",
    "    for row in range(agent.maze.num_rows):\n",
    "        for col in range(agent.maze.num_cols):\n",
    "            agent.value_function[(row, col)] = 0\n",
    "\n",
    "    k = 0\n",
    "    while True:\n",
    "        delta_k = 0\n",
    "        for row in range(agent.maze.num_rows):\n",
    "            for col in range(agent.maze.num_cols):\n",
    "                if (row, col) in agent.maze.terminal_states:\n",
    "                    continue\n",
    "\n",
    "                max_value = float('-inf')\n",
    "                for action in agent.policy.actions.keys():\n",
    "                    next_row, next_col = row + agent.policy.actions[action][0], col + agent.policy.actions[action][1]\n",
    "                    if (0 <= next_row < agent.maze.num_rows) and (0 <= next_col < agent.maze.num_cols):\n",
    "                        next_state = (next_row, next_col)\n",
    "                        # Calculate the value of the next state using the Bellman equation\n",
    "                        next_value = agent.maze.map[next_row][next_col] + gamma * agent.value_function[next_state]\n",
    "                        max_value = max(max_value, next_value)\n",
    "\n",
    "                delta_k = max(delta_k, abs(max_value - agent.value_function[(row, col)]))\n",
    "                agent.value_function[(row, col)] = max_value\n",
    "\n",
    "        k += 1\n",
    "        print(f'k={k}')\n",
    "        print(f'delta={delta_k}')\n",
    "        print_value_function(agent.value_function, agent.policy)\n",
    "        if delta_k < delta:\n",
    "            break\n",
    "\n",
    "\n",
    "def print_value_function(value_function, policy):\n",
    "    \"\"\"\n",
    "    Print the value function and policy.\n",
    "\n",
    "    Args:\n",
    "        value_function (dict): The value function.\n",
    "        policy (Policy): The policy for selecting actions.\n",
    "    \"\"\"\n",
    "    print(\"Utility:\")\n",
    "    for row in range(policy.maze.num_rows):\n",
    "        for col in range(policy.maze.num_cols):\n",
    "            # Print the utility value of each state\n",
    "            print(f'{value_function[(row, col)]:>4}', end=' ')\n",
    "        print()\n",
    "\n",
    "    print(\"Policy:\")\n",
    "    for row in range(policy.maze.num_rows):\n",
    "        for col in range(policy.maze.num_cols):\n",
    "            if (row, col) in policy.maze.terminal_states:\n",
    "                # Print a circle symbol for terminal states\n",
    "                print('○', end=' ')\n",
    "            else:\n",
    "                state = (row, col)\n",
    "                max_value = float('-inf')\n",
    "                best_action = None\n",
    "                for action in policy.actions.keys():\n",
    "                    next_row, next_col = row + policy.actions[action][0], col + policy.actions[action][1]\n",
    "                    if (0 <= next_row < policy.maze.num_rows) and (0 <= next_col < policy.maze.num_cols):\n",
    "                        next_state = (next_row, next_col)\n",
    "                        next_value = value_function[next_state]\n",
    "                        if next_value > max_value:\n",
    "                            max_value = next_value\n",
    "                            best_action = action\n",
    "\n",
    "                # Print the optimal action symbol for each state\n",
    "                print(best_action, end=' ')\n",
    "        print()\n",
    "\n",
    "agent = Agent()\n",
    "value_iteration(agent, delta=0.01, gamma=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
