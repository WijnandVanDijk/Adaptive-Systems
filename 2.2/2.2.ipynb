{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd86e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642b64a",
   "metadata": {},
   "source": [
    "# TD-Learning\n",
    "Opdracht: Implementeer temporal difference learning. Voer de evaluatie uit op de optimale policy π∗ met γ = 1 en γ = 0.5. Visualiseer de uitkomsten en verklaar het resultaat.\n",
    "\n",
    "Note: Werkt nog niet goed, als ik select_action random wil maken duurt het heel lang om te runnen. voor de rest ook buggie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c428582",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Utility (γ = 0.9):\n",
      "-1.0 -1.3 -1.1 0.0 \n",
      "-1.1 -1.0 -9.2 0.6 \n",
      "1.8 1.7 -16.0 -17.9 \n",
      "0.0 -4.3 0.4 -0.4 \n",
      "\n",
      "Policy (γ = 0.9):\n",
      "← ↑ ← × \n",
      "← ← ← ← \n",
      "← ← ← ← \n",
      "× ← ○ ← \n",
      "\n",
      "Utility (γ = 1.0):\n",
      "0.4 0.4 -1.8 0.0 \n",
      "0.4 0.5 -11.4 1.9 \n",
      "2.3 1.5 -15.8 -18.5 \n",
      "9.3 9.7 5.6 4.9 \n",
      "\n",
      "Policy (γ = 1.0):\n",
      "← ← ← × \n",
      "← ← ← ← \n",
      "← ← ← ← \n",
      "× ← ○ ← \n"
     ]
    }
   ],
   "source": [
    "class Maze:\n",
    "    def __init__(self):\n",
    "        self.map = [[-1, -1, -1, +40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [+10, -2, -1, -1]]\n",
    "        self.num_rows = len(self.map)\n",
    "        self.num_cols = len(self.map[0])\n",
    "        self.start_state = (3, 2)  # row, col\n",
    "        self.terminal_states = [(0, 3), (3, 0)]\n",
    "        self.agent_pos = self.start_state\n",
    "        self.actions = {'↑': (-1, 0), '→': (0, 1), '↓': (1, 0), '←': (0, -1)}\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self.actions[action]\n",
    "        row, col = self.agent_pos\n",
    "        new_row, new_col = row + move[0], col + move[1]\n",
    "\n",
    "        # Check if new location is within map boundaries\n",
    "        if (0 <= new_row < self.num_rows) and (0 <= new_col < self.num_cols):\n",
    "            self.agent_pos = (new_row, new_col)\n",
    "            reward = self.map[new_row][new_col]\n",
    "            done = self.agent_pos in self.terminal_states\n",
    "        else:\n",
    "            # If the new location is outside the map, stay in the same spot and get the same punishment again\n",
    "            reward = self.map[row][col]\n",
    "            done = False\n",
    "\n",
    "        return self.agent_pos, reward, done\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, agent, epsilon=0.1):\n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        possible_actions = ['↑', '→', '↓', '←']\n",
    "\n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "\n",
    "        # Exploitation: Choose action with highest utility score\n",
    "        max_utility = float('-inf')\n",
    "        best_action = None\n",
    "\n",
    "        for action in possible_actions:\n",
    "            next_state, reward, done = self.agent.maze.step(action)\n",
    "            utility = self.agent.value_function.get(next_state, 0)\n",
    "            if utility > max_utility:\n",
    "                max_utility = utility\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.maze = Maze()\n",
    "        self.policy = Policy(self)\n",
    "        self.value_function = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state in self.maze.terminal_states:\n",
    "                    self.value_function[state] = 0\n",
    "                else:\n",
    "                    self.value_function[state] = random.random() * 10\n",
    "\n",
    "    def act(self, state):\n",
    "        action = self.policy.select_action(state)\n",
    "        next_state, reward, done = self.maze.step(action)\n",
    "        self.update_value_function(state, next_state, reward)\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_value_function(self, state, next_state, reward):\n",
    "        if next_state in self.maze.terminal_states:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.value_function.get(next_state, 0)\n",
    "\n",
    "        td_error = td_target - self.value_function.get(state, 0)\n",
    "        self.value_function[state] += self.alpha * td_error\n",
    "\n",
    "\n",
    "        \n",
    "    def reset_value_function(self):\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state in self.maze.terminal_states:\n",
    "                    self.value_function[state] = 0\n",
    "                else:\n",
    "                    self.value_function[state] = random.random() * 10\n",
    "                \n",
    "\n",
    "def print_utility(agent, gamma):\n",
    "    print(f\"\\nUtility (γ = {gamma}):\")\n",
    "    state = agent.maze.start_state\n",
    "\n",
    "    for i in range(10):\n",
    "        action = agent.policy.select_action(state)\n",
    "        state, reward, done = agent.act(state)\n",
    "\n",
    "    for row in range(agent.maze.num_rows):\n",
    "        for col in range(agent.maze.num_cols):\n",
    "            state = (row, col)\n",
    "            value = agent.value_function.get(state, 0)\n",
    "            print(f\"{value:.1f}\", end=' ')\n",
    "        print()\n",
    "\n",
    "def print_policy(agent, gamma):\n",
    "    print(f\"\\nPolicy (γ = {gamma}):\")\n",
    "    state = agent.maze.start_state\n",
    "\n",
    "    for i in range(10):\n",
    "        action = agent.policy.select_action(state)\n",
    "        state, reward, done = agent.act(state)\n",
    "\n",
    "    for row in range(agent.maze.num_rows):\n",
    "        for col in range(agent.maze.num_cols):\n",
    "            state = (row, col)\n",
    "            if state == agent.maze.start_state:\n",
    "                print('○', end=' ')\n",
    "            elif state in agent.maze.terminal_states:\n",
    "                print('×', end=' ')\n",
    "            else:\n",
    "                action = agent.policy.select_action(state)\n",
    "                print(action, end=' ')\n",
    "        print()\n",
    "\n",
    "agent = Agent(alpha=0.1, gamma=0.9)\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(10):\n",
    "    state = agent.maze.start_state\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done = agent.act(state)\n",
    "\n",
    "print_utility(agent, gamma=0.9)\n",
    "print_policy(agent, gamma=0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "agent.reset_value_function()\n",
    "\n",
    "\n",
    "\n",
    "# Train again with gamma = 1\n",
    "agent = Agent(alpha=0.1, gamma=1)\n",
    "\n",
    "for episode in range(10):\n",
    "    state = agent.maze.start_state\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done = agent.act(state)\n",
    "\n",
    "print_utility(agent, gamma=1.0)\n",
    "print_policy(agent, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4f880",
   "metadata": {},
   "source": [
    "# Sarsa \n",
    "Opdracht: Implementeer SARSA (on-policy TD control). Voer control uit met γ = 1 en γ = 0.9, visualiseer de uitkomsten en verklaar het resultaat.\n",
    "\n",
    "Antwoord: \n",
    "met γ = 1 leert de agent om optimale acties te kiezen met een langetermijnvisie, terwijl γ = 0.9 leidt tot meer kortetermijndenken. Dit is te zien in de output door hogere utility-waarden en policy die meer (direct) naar het doel bewegen bij γ = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8364a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 250000\n",
      "State (0, 0): (↑): 25.0 (→): 28.7 (↓): 21.1 (←): 25.2   Best: (→): 28.7\n",
      "State (0, 1): (↑): 29.4 (→): 35.0 (↓): 24.1 (←): 24.9   Best: (→): 35.0\n",
      "State (0, 2): (↑): 34.2 (→): 40.0 (↓): 19.6 (←): 29.0   Best: (→): 40.0\n",
      "State (0, 3): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "\n",
      "State (1, 0): (↑): 24.6 (→): 23.6 (↓): 17.8 (←): 20.4   Best: (↑): 24.6\n",
      "State (1, 1): (↑): 30.4 (→): 19.8 (↓): 19.6 (←): 21.4   Best: (↑): 30.4\n",
      "State (1, 2): (↑): 34.9 (→): 24.4 (↓): 17.2 (←): 25.0   Best: (↑): 34.9\n",
      "State (1, 3): (↑): 40.0 (→): 25.4 (↓): 20.0 (←): 19.8   Best: (↑): 40.0\n",
      "\n",
      "State (2, 0): (↑): 21.3 (→): 20.3 (↓): 10.0 (←): 17.9   Best: (↑): 21.3\n",
      "State (2, 1): (↑): 25.6 (→): 17.2 (↓): 16.4 (←): 16.9   Best: (↑): 25.6\n",
      "State (2, 2): (↑): 19.8 (→): 19.7 (↓): 14.2 (←): 21.2   Best: (←): 21.2\n",
      "State (2, 3): (↑): 25.6 (→): 21.2 (↓): 17.4 (←): 17.6   Best: (↑): 25.6\n",
      "\n",
      "State (3, 0): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "State (3, 1): (↑): 21.3 (→): 14.7 (↓): 16.0 (←): 10.0   Best: (↑): 21.3\n",
      "State (3, 2): (↑): 16.8 (→): 17.7 (↓): 14.9 (←): 16.2   Best: (→): 17.7\n",
      "State (3, 3): (↑): 21.7 (→): 18.0 (↓): 17.4 (←): 14.4   Best: (↑): 21.7\n",
      "\n",
      "Utility Map:\n",
      "30.5 35.0 40.0 0.0 \n",
      "24.6 30.5 35.0 40.0 \n",
      "21.1 26.4 21.9 26.0 \n",
      "0.0 22.0 18.5 22.0 \n",
      "Final Policy:\n",
      "Policy (γ = 0.9):\n",
      "→ → → × \n",
      "↑ ↑ ↑ ↑ \n",
      "↑ ↑ ← ↑ \n",
      "× ↑ → ↑ \n",
      "\n",
      "Episodes: 250000\n",
      "State (0, 0): (↑): 35.7 (→): 37.6 (↓): 35.0 (←): 35.9   Best: (→): 37.6\n",
      "State (0, 1): (↑): 36.9 (→): 38.7 (↓): 35.9 (←): 36.2   Best: (→): 38.7\n",
      "State (0, 2): (↑): 38.9 (→): 40.0 (↓): 28.3 (←): 37.0   Best: (→): 40.0\n",
      "State (0, 3): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "\n",
      "State (1, 0): (↑): 36.4 (→): 35.3 (↓): 31.4 (←): 35.2   Best: (↑): 36.4\n",
      "State (1, 1): (↑): 37.5 (→): 28.5 (↓): 34.6 (←): 35.0   Best: (↑): 37.5\n",
      "State (1, 2): (↑): 38.9 (→): 29.0 (↓): 33.4 (←): 35.4   Best: (↑): 38.9\n",
      "State (1, 3): (↑): 40.0 (→): 32.2 (↓): 34.2 (←): 26.2   Best: (↑): 40.0\n",
      "\n",
      "State (2, 0): (↑): 34.7 (→): 34.9 (↓): 10.0 (←): 33.5   Best: (→): 34.9\n",
      "State (2, 1): (↑): 36.3 (→): 33.2 (↓): 32.4 (←): 33.7   Best: (↑): 36.3\n",
      "State (2, 2): (↑): 28.1 (→): 32.5 (↓): 31.9 (←): 34.8   Best: (←): 34.8\n",
      "State (2, 3): (↑): 29.8 (→): 32.0 (↓): 31.4 (←): 33.8   Best: (←): 33.8\n",
      "\n",
      "State (3, 0): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "State (3, 1): (↑): 34.8 (→): 31.9 (↓): 32.4 (←): 10.0   Best: (↑): 34.8\n",
      "State (3, 2): (↑): 33.2 (→): 31.1 (↓): 32.1 (←): 32.1   Best: (↑): 33.2\n",
      "State (3, 3): (↑): 32.4 (→): 31.3 (↓): 31.4 (←): 31.9   Best: (↑): 32.4\n",
      "\n",
      "Utility Map:\n",
      "37.5 39.0 40.0 0.0 \n",
      "36.9 37.7 39.0 40.0 \n",
      "35.0 36.5 35.3 34.2 \n",
      "0.0 35.1 33.8 32.7 \n",
      "Final Policy:\n",
      "Policy (γ = 1):\n",
      "← → → × \n",
      "↑ ↓ ↑ ↑ \n",
      "→ ↑ ← → \n",
      "× ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "class Maze:\n",
    "    def __init__(self):\n",
    "        self.map = [[-1, -1, -1, +40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [+10, -2, -1, -1]]\n",
    "        self.num_rows = len(self.map)\n",
    "        self.num_cols = len(self.map[0])\n",
    "        self.start_state = (3, 2)  # row, col\n",
    "        self.terminal_states = [(0, 3), (3, 0)]\n",
    "        self.agent_pos = self.start_state\n",
    "        self.actions = {'↑': (-1, 0), '→': (0, 1), '↓': (1, 0), '←': (0, -1)}\n",
    "\n",
    "    def step(self, state, action):\n",
    "        move = self.actions[action]\n",
    "        row, col = state\n",
    "        new_row, new_col = row + move[0], col + move[1]\n",
    "        \n",
    "        # Check if new location is within map boundaries\n",
    "        if (0 <= new_row < self.num_rows) and (0 <= new_col < self.num_cols):\n",
    "            next_state = (new_row, new_col)\n",
    "            reward = self.map[new_row][new_col]\n",
    "            done = next_state in self.terminal_states\n",
    "        else:\n",
    "            # If the new location is outside the map, stay in the same spot and get the same punishment again\n",
    "            next_state = state\n",
    "            reward = self.map[row][col]\n",
    "            done = False\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, gamma, learning_rate, epsilon):\n",
    "        self.maze = Maze()\n",
    "        self.q_values = {}\n",
    "        self.utility = {}  # New attribute to store utility values\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Initialize Q-values with random numbers for non-terminal states\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state not in self.maze.terminal_states:\n",
    "                    self.q_values[state] = {action: random.uniform(1, 100) for action in self.maze.actions}\n",
    "\n",
    "    def update_q_value(self, state, action, next_state, next_action, reward):\n",
    "        current_q_value = self.q_values.get(state, {}).get(action, 0)\n",
    "        next_q_value = self.q_values.get(next_state, {}).get(next_action, 0)\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.gamma * next_q_value)\n",
    "        self.q_values[state][action] = new_q_value\n",
    "\n",
    "        # Update utility values based on Q-values\n",
    "        best_action_value = max(self.q_values[next_state].values(), default=0) if next_state in self.q_values else 0\n",
    "        self.utility[state] = reward + self.gamma * best_action_value\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(list(self.maze.actions.keys()))\n",
    "        else:\n",
    "            q_values_for_state = self.q_values.get(state, {})\n",
    "            if q_values_for_state:\n",
    "                max_q_value = max(q_values_for_state.values())\n",
    "                best_actions = [action for action, q_value in q_values_for_state.items() if q_value == max_q_value]\n",
    "                return random.choice(best_actions)\n",
    "            else:\n",
    "                return random.choice(list(self.maze.actions.keys()))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        action = self.select_action(state)\n",
    "        next_state, reward, done = self.maze.step(state, action)\n",
    "        next_action = self.select_action(next_state)\n",
    "        self.update_q_value(state, action, next_state, next_action, reward)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset_q_values(self):\n",
    "        self.q_values = {}\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state not in self.maze.terminal_states:\n",
    "                    self.q_values[state] = {action: random.uniform(1, 100) for action in self.maze.actions}\n",
    "\n",
    "def print_utility(agent, gamma):\n",
    "    maze = agent.maze\n",
    "    q_values = agent.q_values\n",
    "    utility = agent.utility\n",
    "\n",
    "    for i in range(maze.num_rows):\n",
    "        for j in range(maze.num_cols):\n",
    "            state = (i, j)\n",
    "            actions = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "            values = [q_values.get(state, {}).get(action, 0) for action in actions]\n",
    "            best_action = actions[np.argmax(values)]\n",
    "            best_value = np.max(values)\n",
    "\n",
    "            print(f\"State ({i}, {j}):\", end=\" \")\n",
    "            for action, value in zip(actions, values):\n",
    "                print(f\"({action}): {value:.1f}\", end=\" \")\n",
    "            print(f\"  Best: ({best_action}): {best_value:.1f}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Print the 2D map representation of utility\n",
    "    print(\"Utility Map:\")\n",
    "    for i in range(maze.num_rows):\n",
    "        for j in range(maze.num_cols):\n",
    "            state = (i, j)\n",
    "            print(f\"{utility.get(state, 0) * gamma ** 0:.1f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "def print_policy(agent, gamma):\n",
    "    print(f\"Policy (γ = {gamma}):\")\n",
    "    for row in range(agent.maze.num_rows):\n",
    "        for col in range(agent.maze.num_cols):\n",
    "            state = (row, col)\n",
    "            if state == agent.maze.start_state:\n",
    "                best_action = max(agent.q_values[state].keys(), key=lambda k: agent.q_values[state][k])\n",
    "                print(best_action, end=\" \")\n",
    "            elif state in agent.maze.terminal_states:\n",
    "                print(\"×\", end=\" \")\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "                print(action, end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "num_episodes = 250000\n",
    "print_interval = 100\n",
    "\n",
    "# Run for gamma = 0.9\n",
    "agent = Agent(gamma=0.9, learning_rate=0.1, epsilon=0.1)\n",
    "for episode in range(num_episodes):\n",
    "    state = agent.maze.start_state\n",
    "    action = agent.select_action(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done = agent.maze.step(state, action)\n",
    "        next_action = agent.select_action(next_state)\n",
    "        agent.update_q_value(state, action, next_state, next_action, reward)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        \n",
    "    #if episode % print_interval == 0:\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        #print_utility(agent, gamma=0.9)\n",
    "        \n",
    "print(f\"Episodes: {num_episodes}\")\n",
    "print_utility(agent, gamma=0.9)\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print_policy(agent, gamma=0.9)\n",
    "\n",
    "agent.reset_q_values()\n",
    "\n",
    "# Run for gamma = 1\n",
    "agent = Agent(gamma=1, learning_rate=0.1, epsilon=0.1)\n",
    "for episode in range(num_episodes):\n",
    "    state = agent.maze.start_state\n",
    "    action = agent.select_action(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        next_state, reward, done = agent.maze.step(state, action)\n",
    "        next_action = agent.select_action(next_state)\n",
    "        agent.update_q_value(state, action, next_state, next_action, reward)\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        \n",
    "    #if episode % print_interval == 0:\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        #print_utility(agent, gamma=1)\n",
    "print()\n",
    "print(f\"Episodes: {num_episodes}\")\n",
    "print_utility(agent, gamma=1)\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print_policy(agent, gamma=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b0b1",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "Opdracht: Implementeer Q-learning (off-policy TD control). Voer control uit met γ = 1 en γ = 0.9, visualiseer de uitkomsten en verklaar het resultaat.\n",
    "\n",
    "het antwoord op vraag is eigenlijk hetzelfde als bij sarsa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7090266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 250000\n",
      "State (0, 0): (↑): 26.4 (→): 30.5 (↓): 22.8 (←): 26.5   Best: (→): 30.5\n",
      "State (0, 1): (↑): 30.5 (→): 35.0 (↓): 26.4 (←): 26.4   Best: (→): 35.0\n",
      "State (0, 2): (↑): 35.0 (→): 40.0 (↓): 21.5 (←): 30.5   Best: (→): 40.0\n",
      "State (0, 3): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "\n",
      "State (1, 0): (↑): 26.4 (→): 26.4 (↓): 19.5 (←): 22.8   Best: (↑): 26.4\n",
      "State (1, 1): (↑): 30.5 (→): 21.5 (↓): 22.8 (←): 22.8   Best: (↑): 30.5\n",
      "State (1, 2): (↑): 35.0 (→): 26.0 (↓): 19.5 (←): 26.4   Best: (↑): 35.0\n",
      "State (1, 3): (↑): 40.0 (→): 26.0 (↓): 22.4 (←): 21.5   Best: (↑): 40.0\n",
      "\n",
      "State (2, 0): (↑): 22.8 (→): 22.8 (↓): 10.0 (←): 19.5   Best: (↑): 22.8\n",
      "State (2, 1): (↑): 26.4 (→): 19.5 (↓): 18.5 (←): 19.5   Best: (↑): 26.4\n",
      "State (2, 2): (↑): 21.5 (→): 22.4 (↓): 16.6 (←): 22.8   Best: (←): 22.8\n",
      "State (2, 3): (↑): 26.0 (→): 22.4 (↓): 19.2 (←): 19.5   Best: (↑): 26.0\n",
      "\n",
      "State (3, 0): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "State (3, 1): (↑): 22.8 (→): 16.6 (↓): 18.5 (←): 10.0   Best: (↑): 22.8\n",
      "State (3, 2): (↑): 19.5 (→): 19.2 (↓): 16.6 (←): 18.5   Best: (↑): 19.5\n",
      "State (3, 3): (↑): 22.4 (→): 19.2 (↓): 19.2 (←): 16.6   Best: (↑): 22.4\n",
      "\n",
      "Utility Map:\n",
      "30.5 35.0 40.0 0.0 \n",
      "26.4 30.5 35.0 40.0 \n",
      "22.8 26.4 22.8 26.0 \n",
      "0.0 22.8 19.5 22.4 \n",
      "Final Policy:\n",
      "Policy (γ = 0.9):\n",
      "→ → → × \n",
      "↑ ↑ ↑ ↑ \n",
      "↑ ↓ ← ↑ \n",
      "× ↑ ↑ ↑ \n",
      "\n",
      "Episodes: 250000\n",
      "State (0, 0): (↑): 37.0 (→): 38.0 (↓): 36.0 (←): 37.0   Best: (→): 38.0\n",
      "State (0, 1): (↑): 38.0 (→): 39.0 (↓): 37.0 (←): 37.0   Best: (→): 39.0\n",
      "State (0, 2): (↑): 39.0 (→): 40.0 (↓): 29.0 (←): 38.0   Best: (→): 40.0\n",
      "State (0, 3): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "\n",
      "State (1, 0): (↑): 37.0 (→): 37.0 (↓): 35.0 (←): 36.0   Best: (↑): 37.0\n",
      "State (1, 1): (↑): 38.0 (→): 29.0 (↓): 36.0 (←): 36.0   Best: (↑): 38.0\n",
      "State (1, 2): (↑): 39.0 (→): 30.0 (↓): 35.0 (←): 37.0   Best: (↑): 39.0\n",
      "State (1, 3): (↑): 40.0 (→): 30.2 (↓): 34.9 (←): 28.7   Best: (↑): 40.0\n",
      "\n",
      "State (2, 0): (↑): 36.0 (→): 36.0 (↓): 10.0 (←): 35.0   Best: (↑): 36.0\n",
      "State (2, 1): (↑): 37.0 (→): 35.0 (↓): 34.0 (←): 35.0   Best: (↑): 37.0\n",
      "State (2, 2): (↑): 29.0 (→): 34.0 (↓): 34.0 (←): 36.0   Best: (←): 36.0\n",
      "State (2, 3): (↑): 30.0 (→): 34.0 (↓): 33.0 (←): 35.0   Best: (←): 35.0\n",
      "\n",
      "State (3, 0): (↑): 0.0 (→): 0.0 (↓): 0.0 (←): 0.0   Best: (↑): 0.0\n",
      "State (3, 1): (↑): 36.0 (→): 34.0 (↓): 34.0 (←): 10.0   Best: (↑): 36.0\n",
      "State (3, 2): (↑): 35.0 (→): 33.0 (↓): 34.0 (←): 34.0   Best: (↑): 35.0\n",
      "State (3, 3): (↑): 34.0 (→): 33.0 (↓): 33.0 (←): 34.0   Best: (↑): 34.0\n",
      "\n",
      "Utility Map:\n",
      "38.0 39.0 40.0 0.0 \n",
      "37.0 38.0 39.0 40.0 \n",
      "36.0 37.0 36.0 35.0 \n",
      "0.0 36.0 35.0 34.0 \n",
      "Final Policy:\n",
      "Policy (γ = 1):\n",
      "→ ← → × \n",
      "→ ↑ ↑ ↑ \n",
      "→ ↑ ← ← \n",
      "× ↑ ↑ ← \n"
     ]
    }
   ],
   "source": [
    "class Maze:\n",
    "    def __init__(self):\n",
    "        self.map = [[-1, -1, -1, +40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [+10, -2, -1, -1]]\n",
    "        self.num_rows = len(self.map)\n",
    "        self.num_cols = len(self.map[0])\n",
    "        self.start_state = (3, 2)  # row, col\n",
    "        self.terminal_states = [(0, 3), (3, 0)]\n",
    "        self.agent_pos = self.start_state\n",
    "        self.actions = {'↑': (-1, 0), '→': (0, 1), '↓': (1, 0), '←': (0, -1)}\n",
    "\n",
    "    def step(self, state, action):\n",
    "        move = self.actions[action]\n",
    "        row, col = state\n",
    "        new_row, new_col = row + move[0], col + move[1]\n",
    "        \n",
    "        # Check if new location is within map boundaries\n",
    "        if (0 <= new_row < self.num_rows) and (0 <= new_col < self.num_cols):\n",
    "            next_state = (new_row, new_col)\n",
    "            reward = self.map[new_row][new_col]\n",
    "            done = next_state in self.terminal_states\n",
    "        else:\n",
    "            # If the new location is outside the map, stay in the same spot and get the same punishment again\n",
    "            next_state = state\n",
    "            reward = self.map[row][col]\n",
    "            done = False\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, gamma, learning_rate, epsilon):\n",
    "        self.maze = Maze()\n",
    "        self.q_values = {}\n",
    "        self.utility = {}\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state not in self.maze.terminal_states:\n",
    "                    self.q_values[state] = {action: random.uniform(1, 100) for action in self.maze.actions}\n",
    "                    self.utility[state] = 0\n",
    "\n",
    "    def update_q_value(self, state, action, next_state, reward):\n",
    "        max_next_q_value = max(self.q_values[next_state].values(), default=0) if next_state in self.q_values else 0\n",
    "        current_q_value = self.q_values[state].get(action, 0)\n",
    "        new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.gamma * max_next_q_value)\n",
    "        self.q_values[state][action] = new_q_value\n",
    "\n",
    "        # Update utility value\n",
    "        self.utility[state] = max(self.q_values[state].values())\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(list(self.maze.actions.keys()))\n",
    "        else:\n",
    "            q_values_for_state = self.q_values.get(state, {})\n",
    "            if q_values_for_state:\n",
    "                max_q_value = max(q_values_for_state.values())\n",
    "                best_actions = [action for action, q_value in q_values_for_state.items() if q_value == max_q_value]\n",
    "                return random.choice(best_actions)\n",
    "            else:\n",
    "                return random.choice(list(self.maze.actions.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        action = self.select_action(state)\n",
    "        next_state, reward, done = self.maze.step(state, action)\n",
    "        self.update_q_value(state, action, next_state, reward)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset_q_values(self):\n",
    "        self.q_values = {}\n",
    "        for row in range(self.maze.num_rows):\n",
    "            for col in range(self.maze.num_cols):\n",
    "                state = (row, col)\n",
    "                if state not in self.maze.terminal_states:\n",
    "                    self.q_values[state] = {action: random.uniform(1, 100) for action in self.maze.actions}\n",
    "\n",
    "def print_utility(agent, gamma):\n",
    "    maze = agent.maze\n",
    "    q_values = agent.q_values\n",
    "    utility = agent.utility\n",
    "\n",
    "    for i in range(maze.num_rows):\n",
    "        for j in range(maze.num_cols):\n",
    "            state = (i, j)\n",
    "            actions = [\"↑\", \"→\", \"↓\", \"←\"]\n",
    "            values = [q_values.get(state, {}).get(action, 0) for action in actions]\n",
    "            best_action = actions[np.argmax(values)]\n",
    "            best_value = np.max(values)\n",
    "\n",
    "            print(f\"State ({i}, {j}):\", end=\" \")\n",
    "            for action, value in zip(actions, values):\n",
    "                print(f\"({action}): {value:.1f}\", end=\" \")\n",
    "            print(f\"  Best: ({best_action}): {best_value:.1f}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Print the 2D map representation of utility\n",
    "    print(\"Utility Map:\")\n",
    "    for i in range(maze.num_rows):\n",
    "        for j in range(maze.num_cols):\n",
    "            state = (i, j)\n",
    "            print(f\"{utility.get(state, 0) * gamma ** 0:.1f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def print_policy(agent, gamma):\n",
    "    print(f\"Policy (γ = {gamma}):\")\n",
    "    for row in range(agent.maze.num_rows):\n",
    "        for col in range(agent.maze.num_cols):\n",
    "            state = (row, col)\n",
    "            if state == agent.maze.start_state:\n",
    "                best_action = max(agent.q_values[state].keys(), key=lambda k: agent.q_values[state][k])\n",
    "                print(best_action, end=\" \")\n",
    "            elif state in agent.maze.terminal_states:\n",
    "                print(\"×\", end=\" \")\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "                print(action, end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "num_episodes = 250000\n",
    "print_interval = 100\n",
    "\n",
    "# Run for gamma = 0.9\n",
    "agent = Agent(gamma=0.9, learning_rate=0.1, epsilon=0.1)\n",
    "for episode in range(num_episodes):\n",
    "    state = agent.maze.start_state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state, reward, done = agent.act(state)\n",
    "        \n",
    "    #if episode % print_interval == 0:\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        #print_utility(agent, gamma=0.9)      \n",
    "print(f\"Episodes: {num_episodes}\")\n",
    "print_utility(agent, gamma=0.9)\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print_policy(agent, gamma=0.9)\n",
    "\n",
    "agent.reset_q_values()\n",
    "\n",
    "# Run for gamma = 1\n",
    "agent = Agent(gamma=1, learning_rate=0.1, epsilon=0.1)\n",
    "for episode in range(num_episodes):\n",
    "    state = agent.maze.start_state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state, reward, done = agent.act(state)\n",
    "        \n",
    "    #if episode % print_interval == 0:\n",
    "        #print(f\"Episode: {episode}\")\n",
    "        #print_utility(agent, gamma=1)\n",
    "print()\n",
    "print(f\"Episodes: {num_episodes}\")\n",
    "print_utility(agent, gamma=1)\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print_policy(agent, gamma=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea994b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
